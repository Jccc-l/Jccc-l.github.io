<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width">
<meta name="theme-color" content="#222"><meta name="generator" content="Hexo 7.3.0">

  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">



<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css" integrity="sha256-XOqroi11tY4EFQMR9ZYwZWKj5ZXiftSx36RRuC3anlA=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/animate.css/3.1.1/animate.min.css" integrity="sha256-PR7ttpcvz8qrF57fur/yAx1qXMFJeJFiA6pSzWi0OIE=" crossorigin="anonymous">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/themes/blue/pace-theme-minimal.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/pace/1.2.4/pace.min.js" integrity="sha256-gqd7YTjg/BtfqWSwsJOvndl0Bxc8gFImLEkXQT8+qj0=" crossorigin="anonymous"></script>

<script class="next-config" data-name="main" type="application/json">{"hostname":"jccc-l.github.io","root":"/","images":"/images","scheme":"Gemini","darkmode":false,"version":"8.20.0","exturl":false,"sidebar":{"position":"left","width_expanded":320,"width_dual_column":240,"display":"hide","padding":18,"offset":12},"copycode":{"enable":true,"style":"mac"},"fold":{"enable":true,"height":500},"bookmark":{"enable":false,"color":"#222","save":"auto"},"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"stickytabs":false,"motion":{"enable":true,"async":false,"transition":{"menu_item":"fadeInDown","post_block":"fadeIn","post_header":"fadeInDown","post_body":"fadeInDown","coll_header":"fadeInLeft","sidebar":"fadeInUp"}},"i18n":{"placeholder":"搜索...","empty":"没有找到任何搜索结果：${query}","hits_time":"找到 ${hits} 个搜索结果（用时 ${time} 毫秒）","hits":"找到 ${hits} 个搜索结果"},"path":"/search.xml","localsearch":{"enable":true,"top_n_per_article":1,"unescape":false,"preload":false}}</script><script src="/js/config.js"></script>

    <meta name="description" content="林子雨老师的网站用户行为分析案例的复现记录">
<meta property="og:type" content="article">
<meta property="og:title" content="大数据实验踩坑指南_No.6——实验案例：网站用户行为分析">
<meta property="og:url" content="https://jccc-l.github.io/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-6%E2%80%94%E2%80%94%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B%EF%BC%9A%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/index.html">
<meta property="og:site_name" content="璐濄のβlòɡ">
<meta property="og:description" content="林子雨老师的网站用户行为分析案例的复现记录">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2024-06-16T08:31:29.000Z">
<meta property="article:modified_time" content="2024-11-13T03:19:05.409Z">
<meta property="article:author" content="Jccc">
<meta property="article:tag" content="Linux">
<meta property="article:tag" content="Big Data">
<meta property="article:tag" content="Hadoop">
<meta property="article:tag" content="ZooKeeper">
<meta property="article:tag" content="HBase">
<meta property="article:tag" content="R">
<meta property="article:tag" content="Hive">
<meta property="article:tag" content="Java">
<meta name="twitter:card" content="summary">


<link rel="canonical" href="https://jccc-l.github.io/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-6%E2%80%94%E2%80%94%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B%EF%BC%9A%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/">



<script class="next-config" data-name="page" type="application/json">{"sidebar":"","isHome":false,"isPost":true,"lang":"zh-CN","comments":true,"permalink":"https://jccc-l.github.io/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-6%E2%80%94%E2%80%94%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B%EF%BC%9A%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/","path":"2024/06/16/大数据实验踩坑指南-No-6——实验案例：网站用户行为分析/","title":"大数据实验踩坑指南_No.6——实验案例：网站用户行为分析"}</script>

<script class="next-config" data-name="calendar" type="application/json">""</script>
<title>大数据实验踩坑指南_No.6——实验案例：网站用户行为分析 | 璐濄のβlòɡ</title>
  








  <noscript>
    <link rel="stylesheet" href="/css/noscript.css">
  </noscript>
</head>

<body itemscope itemtype="http://schema.org/WebPage" class="use-motion">
  <div class="headband"></div>

  <main class="main">
    <div class="column">
      <header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏" role="button">
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
        <span class="toggle-line"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <i class="logo-line"></i>
      <p class="site-title">璐濄のβlòɡ</p>
      <i class="logo-line"></i>
    </a>
      <img class="custom-logo-image" src="https://static.wixstatic.com/media/7ac599_2e792bfa812140c898f6c3a78e4ab78f~mv2.png/v1/fill/w_951,h_1046,al_c,q_90,usm_0.66_1.00_0.01,enc_auto/7ac599_2e792bfa812140c898f6c3a78e4ab78f~mv2.png" alt="璐濄のβlòɡ">
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger" aria-label="搜索" role="button">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>



<nav class="site-nav">
  <ul class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-about"><a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
      <div class="search-header">
        <span class="search-icon">
          <i class="fa fa-search"></i>
        </span>
        <div class="search-input-container">
          <input autocomplete="off" autocapitalize="off" maxlength="80"
                placeholder="搜索..." spellcheck="false"
                type="search" class="search-input">
        </div>
        <span class="popup-btn-close" role="button">
          <i class="fa fa-times-circle"></i>
        </span>
      </div>
      <div class="search-result-container">
        <div class="search-result-icon">
          <i class="fa fa-spinner fa-pulse fa-5x"></i>
        </div>
      </div>
    </div>
  </div>

</header>
        
  
  <aside class="sidebar">

    <div class="sidebar-inner sidebar-nav-active sidebar-toc-active">
      <ul class="sidebar-nav">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <div class="sidebar-panel-container">
        <!--noindex-->
        <div class="post-toc-wrap sidebar-panel">
            <div class="post-toc animated"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97_no6%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90"><span class="nav-text"> 大数据实验踩坑指南_No.6——实验案例：网站用户行为分析</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA"><span class="nav-text"> 实验环境搭建</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%AC%E5%9C%B0%E6%95%B0%E6%8D%AE%E9%9B%86%E4%B8%8A%E4%BC%A0%E5%88%B0%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93hive"><span class="nav-text"> 本地数据集上传到数据仓库Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E9%AA%8C%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E4%B8%8B%E8%BD%BD"><span class="nav-text"> 实验数据集的下载</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E7%9A%84%E9%A2%84%E5%A4%84%E7%90%86"><span class="nav-text"> 数据集的预处理</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AF%BC%E5%85%A5%E6%95%B0%E6%8D%AE%E5%BA%93"><span class="nav-text"> 导入数据库</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hive%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90"><span class="nav-text"> Hive数据分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AE%80%E5%8D%95%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90"><span class="nav-text"> 简单查询分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E8%AF%A2%E6%9D%A1%E6%95%B0%E7%BB%9F%E8%AE%A1"><span class="nav-text"> 查询条数统计</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%85%B3%E9%94%AE%E5%AD%97%E6%9D%A1%E4%BB%B6%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90"><span class="nav-text"> 关键字条件查询分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%A0%B9%E6%8D%AE%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90"><span class="nav-text"> 根据用户行为分析</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%94%A8%E6%88%B7%E5%AE%9E%E6%97%B6%E6%9F%A5%E8%AF%A2%E5%88%86%E6%9E%90"><span class="nav-text"> 用户实时查询分析</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#hive-mysql-hbase%E6%95%B0%E6%8D%AE%E4%BA%92%E5%AF%BC"><span class="nav-text"> Hive、MySQL、HBase数据互导</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#hive%E9%A2%84%E6%93%8D%E4%BD%9C"><span class="nav-text"> Hive预操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8java-api%E5%B0%86%E6%95%B0%E6%8D%AE%E4%BB%8Ehive%E5%AF%BC%E5%85%A5mysql"><span class="nav-text"> 使用Java API将数据从Hive导入MySQL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8hbase-java-api%E6%8A%8A%E6%95%B0%E6%8D%AE%E4%BB%8E%E6%9C%AC%E5%9C%B0%E5%AF%BC%E5%85%A5%E5%88%B0hbase%E4%B8%AD"><span class="nav-text"> 使用HBase Java API把数据从本地导入到HBase中</span></a></li></ol></li></ol></li></ol></div>
        </div>
        <!--/noindex-->

        <div class="site-overview-wrap sidebar-panel">
          <div class="site-author animated" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="Jccc"
      src="https://static.wixstatic.com/media/7ac599_2e792bfa812140c898f6c3a78e4ab78f~mv2.png/v1/fill/w_951,h_1046,al_c,q_90,usm_0.66_1.00_0.01,enc_auto/7ac599_2e792bfa812140c898f6c3a78e4ab78f~mv2.png">
  <p class="site-author-name" itemprop="name">Jccc</p>
  <div class="site-description" itemprop="description">Jccc的胡言乱语</div>
</div>
<div class="site-state-wrap animated">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
        <a href="/archives/">
          <span class="site-state-item-count">36</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
          <a href="/categories/">
        <span class="site-state-item-count">10</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
          <a href="/tags/">
        <span class="site-state-item-count">48</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author animated">
      <span class="links-of-author-item">
        <a href="https://github.com/Jccc-l" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;Jccc-l" rel="noopener me" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:1216550215Jc@gmail.com" title="E-Mail → mailto:1216550215Jc@gmail.com" rel="noopener me" target="_blank"><i class="fa fa-envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://huggingface.co/Jccc-l" title="🤗 Hg Face → https:&#x2F;&#x2F;huggingface.co&#x2F;Jccc-l" rel="noopener me" target="_blank">🤗 Hg Face</a>
      </span>
      <span class="links-of-author-item">
        <a href="/contact/WeChat" title="WeChat → &#x2F;contact&#x2F;WeChat" rel="noopener me"><i class="fab fa-weixin fa-fw"></i>WeChat</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://qm.qq.com/q/ZOA0HXIwU0" title="QQ → https:&#x2F;&#x2F;qm.qq.com&#x2F;q&#x2F;ZOA0HXIwU0" rel="noopener me" target="_blank"><i class="fab fa-qq fa-fw"></i>QQ</a>
      </span>
  </div>

        </div>
      </div>
        <div class="back-to-top animated" role="button" aria-label="返回顶部">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
    </div>

    
    <div class="sidebar-inner sidebar-blogroll">
      <div class="links-of-blogroll animated">
        <div class="links-of-blogroll-title"><i class="fa fa-globe fa-fw"></i>
          链接
        </div>
        <ul class="links-of-blogroll-list">
            <li class="links-of-blogroll-item">
              <a href="https://julintongxue.github.io/" title="https:&#x2F;&#x2F;julintongxue.github.io" rel="noopener" target="_blank">orangeの博客</a>
            </li>
            <li class="links-of-blogroll-item">
              <a href="https://georgenhj.github.io/" title="https:&#x2F;&#x2F;georgenhj.github.io&#x2F;" rel="noopener" target="_blank">George Nong's Blog</a>
            </li>
        </ul>
      </div>
    </div>
    <embed id="musicplayer" frameborder="no" border="0" marginwidth="0" marginheight="0" width=400 height=86 src="//music.163.com/outchain/player?type=2&id=407450660&auto=1&height=66"></embed>
  </aside>


    </div>

    <div class="main-inner post posts-expand">


  


<div class="post-block">
  
  

  <article itemscope itemtype="http://schema.org/Article" class="post-content" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jccc-l.github.io/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-6%E2%80%94%E2%80%94%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B%EF%BC%9A%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="https://static.wixstatic.com/media/7ac599_2e792bfa812140c898f6c3a78e4ab78f~mv2.png/v1/fill/w_951,h_1046,al_c,q_90,usm_0.66_1.00_0.01,enc_auto/7ac599_2e792bfa812140c898f6c3a78e4ab78f~mv2.png">
      <meta itemprop="name" content="Jccc">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="璐濄のβlòɡ">
      <meta itemprop="description" content="Jccc的胡言乱语">
    </span>

    <span hidden itemprop="post" itemscope itemtype="http://schema.org/CreativeWork">
      <meta itemprop="name" content="大数据实验踩坑指南_No.6——实验案例：网站用户行为分析 | 璐濄のβlòɡ">
      <meta itemprop="description" content="林子雨老师的网站用户行为分析案例的复现记录">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          大数据实验踩坑指南_No.6——实验案例：网站用户行为分析
        </h1>

        <div class="post-meta-container">
          <div class="post-meta">
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar"></i>
      </span>
      <span class="post-meta-item-text">发表于</span>

      <time title="创建时间：2024-06-16 16:31:29" itemprop="dateCreated datePublished" datetime="2024-06-16T16:31:29+08:00">2024-06-16</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-calendar-check"></i>
      </span>
      <span class="post-meta-item-text">更新于</span>
      <time title="修改时间：2024-11-13 11:19:05" itemprop="dateModified" datetime="2024-11-13T11:19:05+08:00">2024-11-13</time>
    </span>
    <span class="post-meta-item">
      <span class="post-meta-item-icon">
        <i class="far fa-folder"></i>
      </span>
      <span class="post-meta-item-text">分类于</span>
        <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
          <a href="/categories/Big-Data/" itemprop="url" rel="index"><span itemprop="name">Big Data</span></a>
        </span>
    </span>

  
    <span class="post-meta-break"></span>
    <span class="post-meta-item" title="本文字数">
      <span class="post-meta-item-icon">
        <i class="far fa-file-word"></i>
      </span>
      <span class="post-meta-item-text">本文字数：</span>
      <span>38k</span>
    </span>
    <span class="post-meta-item" title="阅读时长">
      <span class="post-meta-item-icon">
        <i class="far fa-clock"></i>
      </span>
      <span class="post-meta-item-text">阅读时长 &asymp;</span>
      <span>35 分钟</span>
    </span>
</div>

            <div class="post-description">林子雨老师的网站用户行为分析案例的复现记录</div>
        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody"><h1 id="大数据实验踩坑指南_no6实验案例网站用户行为分析"><a class="markdownIt-Anchor" href="#大数据实验踩坑指南_no6实验案例网站用户行为分析"></a> 大数据实验踩坑指南_No.6——实验案例：网站用户行为分析</h1>
<h2 id="实验环境搭建"><a class="markdownIt-Anchor" href="#实验环境搭建"></a> 实验环境搭建</h2>
<p>在这个实验中，需要用到下列软件包，以下是我的软件版本，点击即可跳转到官网或国内镜像站的下载链接：</p>
<ul>
<li><a target="_blank" rel="noopener" href="https://mirrors.tuna.tsinghua.edu.cn/ubuntu-releases/22.04/ubuntu-22.04.4-desktop-amd64.iso">Linux</a>: Ubuntu 22.04.4 LTS</li>
<li><a target="_blank" rel="noopener" href="https://dlcdn.apache.org/hadoop/common/hadoop-3.3.6/hadoop-3.3.6.tar.gz">Hadoop</a>: 3.3.6</li>
<li><a target="_blank" rel="noopener" href="https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz">Hive</a>: 3.1.3</li>
<li><a target="_blank" rel="noopener" href="https://dlcdn.apache.org/hive/hive-3.1.3/apache-hive-3.1.3-bin.tar.gz">ZooKeeper</a>: 3.9.2</li>
<li><a target="_blank" rel="noopener" href="https://cran.r-project.org/index.html">R</a>有两种安装方式，效果是一样的，二选一即可
<ul>
<li><a target="_blank" rel="noopener" href="https://cran.r-project.org/bin/linux/ubuntu/">Ubuntu Packages For R - Brief Instructions</a></li>
<li><a target="_blank" rel="noopener" href="https://cran.r-project.org/bin/linux/ubuntu/fullREADME.html">Ubuntu Packages For R - Full Instructions</a></li>
</ul>
</li>
<li><a target="_blank" rel="noopener" href="https://www.apache.org/dyn/closer.lua/hbase/2.5.8/hbase-2.5.8-hadoop3-bin.tar.gz">HBase</a>: 2.5.8-hadoop3-bin</li>
<li><a target="_blank" rel="noopener" href="https://www.jetbrains.com/help/idea/installation-guide.html">JetBrains IntelliJ IDEA</a>
<ul>
<li>使用Linux版本的Standalone installation单独下载IDEA</li>
</ul>
</li>
</ul>
<p>请根据前面的教程安装配置好这些软件</p>
<ul>
<li>
<a href="/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-1%E2%80%94%E2%80%94Hadoop%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/" title="大数据实验踩坑指南_No.1——Hadoop的安装配置">大数据实验踩坑指南_No.1——Hadoop的安装配置</a>
</li>
<li>
<a href="/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-2%E2%80%94%E2%80%94Hive%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/" title="大数据实验踩坑指南_No.2——Hive的安装配置">大数据实验踩坑指南_No.2——Hive的安装配置</a>
</li>
<li>
<a href="/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-3%E2%80%94%E2%80%94ZooKeeper%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/" title="大数据实验踩坑指南_No.3——ZooKeeper的安装配置.md">大数据实验踩坑指南_No.3——ZooKeeper的安装配置.md</a>
</li>
<li>
<a href="/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-4%E2%80%94%E2%80%94HBase%E7%9A%84%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/" title="大数据实验踩坑指南_No.4——HBase的安装配置.md">大数据实验踩坑指南_No.4——HBase的安装配置.md</a>
</li>
<li>
<a href="/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-5%E2%80%94%E2%80%94R%E8%AF%AD%E8%A8%80%E5%92%8C%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%9A%84%E5%AE%89%E8%A3%85/" title="大数据实验踩坑指南_No.5——R语言和软件包的安装">大数据实验踩坑指南_No.5——R语言和软件包的安装</a>
</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://dblab.xmu.edu.cn/blog/4189/">林子雨老师的教程</a></p>
<p>IDE可以自选IntelliJ IDEA<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup>或Eclipse<sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup></p>
<h2 id="本地数据集上传到数据仓库hive"><a class="markdownIt-Anchor" href="#本地数据集上传到数据仓库hive"></a> 本地数据集上传到数据仓库Hive</h2>
<h3 id="实验数据集的下载"><a class="markdownIt-Anchor" href="#实验数据集的下载"></a> 实验数据集的下载</h3>
<ul>
<li>本案例采用的数据集为<code>user.zip</code>，包含了一个大规模数据集<code>raw_user.csv</code>（包含2000万条记录），和一个小数据集<code>small_user.csv</code>（只包含30万条记录）。小数据集<code>small_user.csv</code>是从大规模数据集<code>raw_user.csv</code>中抽取的一小部分数据。之所以抽取出一少部分记录单独构成一个小数据集，是因为在第一遍跑通整个实验流程时，会遇到各种错误、各种问题，先用小数据集测试，可以大量节约程序运行时间。等到第一次完整实验流程都顺利跑通以后，可以最后用大规模数据集进行最后的测试。</li>
<li>把数据集<code>user.zip</code>文件下载到Linux系统的<code>/home/hadoop/Downloads/</code>目录下面</li>
</ul>
<p>下载案例的数据集<code>user.zip</code>到目录<code>/home/hadoop/Downloads/</code>内（系统语言如果是中文，则是`/home/hadoop/下载）</p>
<p>创建一个存储数据集的目录，将<code>user.zip</code>进行解压缩</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> <span class="built_in">mkdir</span> -p /usr/local/bigdatacase/dataset</span><br><span class="line"><span class="built_in">sudo</span> <span class="built_in">chown</span> -R hadoop:hadoop /usr/local/bigdatacase <span class="comment"># 修改bigdatacase目录权限</span></span><br><span class="line">unzip /home/hadoop/Downloads/user.zip -d /usr/local/bigdatacase/dataset <span class="comment"># 解压缩数据文件user.zip</span></span><br><span class="line"><span class="built_in">cd</span> /usr/local/bigdatacase/dataset</span><br></pre></td></tr></table></figure>
<p>现在可以看到<code>dataset</code>目录下有两个文件：<code>raw_user.csv</code>和<code>small_user.csv</code>。执行以下命令查看前5条数据</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">head</span> -5 raw_user.csv</span><br></pre></td></tr></table></figure>
<h3 id="数据集的预处理"><a class="markdownIt-Anchor" href="#数据集的预处理"></a> 数据集的预处理</h3>
<p>删除文件的第一行记录（字段名称）</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sed -i <span class="string">&#x27;1d&#x27;</span> raw_user.csv</span><br><span class="line">sed -i <span class="string">&#x27;1d&#x27;</span> small_user.csv</span><br><span class="line"><span class="built_in">head</span> -5 raw_user.csv</span><br><span class="line"><span class="built_in">head</span> -5 small_user.csv</span><br></pre></td></tr></table></figure>
<p>查看到已经看不到字段名成这一行</p>
<p>对字段进行预处理</p>
<p>下面要建一个脚本文件<code>pre_deal.sh</code>，请把这个脚本文件放在<code>dataset</code>目录下，和数据集<code>raw_user.csv</code>放在同一个目录下：</p>
<figure class="highlight sh"><figcaption><span>/usr/local/bigdatacase/dataset/pre_deal.sh</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"><span class="comment">#下面设置输入文件，把用户执行pre_deal.sh命令时提供的第一个参数作为输入文件名称</span></span><br><span class="line">infile=<span class="variable">$1</span></span><br><span class="line"><span class="comment">#下面设置输出文件，把用户执行pre_deal.sh命令时提供的第二个参数作为输出文件名称</span></span><br><span class="line">outfile=<span class="variable">$2</span></span><br><span class="line"><span class="comment">#注意，最后的$infile&gt; $outfile必须跟在&#125;’这两个字符的后面</span></span><br><span class="line">awk -F <span class="string">&quot;,&quot;</span> <span class="string">&#x27;BEGIN&#123;</span></span><br><span class="line"><span class="string">srand();</span></span><br><span class="line"><span class="string">        id=0;</span></span><br><span class="line"><span class="string">        Province[0]=&quot;山东&quot;;Province[1]=&quot;山西&quot;;Province[2]=&quot;河南&quot;;Province[3]=&quot;河北&quot;;Province[4]=&quot;陕西&quot;;Province[5]=&quot;内蒙古&quot;;Province[6]=&quot;上海市&quot;;</span></span><br><span class="line"><span class="string">        Province[7]=&quot;北京市&quot;;Province[8]=&quot;重庆市&quot;;Province[9]=&quot;天津市&quot;;Province[10]=&quot;福建&quot;;Province[11]=&quot;广东&quot;;Province[12]=&quot;广西&quot;;Province[13]=&quot;云南&quot;; </span></span><br><span class="line"><span class="string">        Province[14]=&quot;浙江&quot;;Province[15]=&quot;贵州&quot;;Province[16]=&quot;新疆&quot;;Province[17]=&quot;西藏&quot;;Province[18]=&quot;江西&quot;;Province[19]=&quot;湖南&quot;;Province[20]=&quot;湖北&quot;;</span></span><br><span class="line"><span class="string">        Province[21]=&quot;黑龙江&quot;;Province[22]=&quot;吉林&quot;;Province[23]=&quot;辽宁&quot;; Province[24]=&quot;江苏&quot;;Province[25]=&quot;甘肃&quot;;Province[26]=&quot;青海&quot;;Province[27]=&quot;四川&quot;;</span></span><br><span class="line"><span class="string">        Province[28]=&quot;安徽&quot;; Province[29]=&quot;宁夏&quot;;Province[30]=&quot;海南&quot;;Province[31]=&quot;香港&quot;;Province[32]=&quot;澳门&quot;;Province[33]=&quot;台湾&quot;;</span></span><br><span class="line"><span class="string">    &#125;</span></span><br><span class="line"><span class="string">    &#123;</span></span><br><span class="line"><span class="string">        id=id+1;</span></span><br><span class="line"><span class="string">        value=int(rand()*34);       </span></span><br><span class="line"><span class="string">        print id&quot;\t&quot;$1&quot;\t&quot;$2&quot;\t&quot;$3&quot;\t&quot;$5&quot;\t&quot;substr($6,1,10)&quot;\t&quot;Province[value]</span></span><br><span class="line"><span class="string">    &#125;&#x27;</span> <span class="variable">$infile</span>&gt; <span class="variable">$outfile</span></span><br></pre></td></tr></table></figure>
<p>下面就可以执行pre_deal.sh脚本文件，来对raw_user.csv进行数据预处理，命令如下：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /usr/local/bigdatacase/dataset</span><br><span class="line">bash ./pre_deal.sh raw_user.csv user_table.txt</span><br></pre></td></tr></table></figure>
<p>可以使用head命令查看前10行数据</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">head</span> -10 user_table.txt</span><br></pre></td></tr></table></figure>
<h3 id="导入数据库"><a class="markdownIt-Anchor" href="#导入数据库"></a> 导入数据库</h3>
<p><strong>启动HDFS</strong></p>
<p>执行下面的命令启动Hadoop：</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br></pre></td></tr></table></figure>
<p>然后输入jps命令查看当前的进程：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">43529 NameNode</span><br><span class="line">43692 DataNode</span><br><span class="line">63644 Jps</span><br><span class="line">43901 SecondaryNameNode</span><br></pre></td></tr></table></figure>
<p><strong>把user_table.txt上传到HDFS中</strong></p>
<p>在HDFS的根目录下创建一个新的目录bigdatacase并在这个目录下创建一个子目录<code>dataset</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">mkdir</span> -p /bigdatacase/dataset</span><br></pre></td></tr></table></figure>
<p>然后把本地的user_table.txt上传到分布式文件系统HDFS的<code>/bigdatacase/dataset</code>目录下</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -put /usr/local/bigdatacase/dataset/user_table.txt /bigdatacase/dataset</span><br></pre></td></tr></table></figure>
<p>可以查看以下HDFS中<code>user_table.txt</code>的前10条记录</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">cat</span> /bigdatacase/dataset/user_table.txt | <span class="built_in">head</span> -10</span><br></pre></td></tr></table></figure>
<p><strong>在Hive上创建数据库</strong></p>
<p>首先启动MySQL数据库</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">service mysql start</span><br></pre></td></tr></table></figure>
<p>启动hive的元数据</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service metastore</span><br></pre></td></tr></table></figure>
<p>在新的终端下进入Hive</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive</span><br></pre></td></tr></table></figure>
<p>在Hive中创建一个数据库dblab</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create database dblab;</span><br><span class="line">hive&gt; use dblab;</span><br></pre></td></tr></table></figure>
<p><strong>创建外部表</strong></p>
<p>在hive命令提示符下输入</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; CREATE EXTERNAL TABLE dblab.bigdata_user(id INT,uid STRING,item_id STRING,behavior_type INT,item_category STRING,visit_date DATE,province STRING) COMMENT &#x27;Welcome to xmudblab!&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; STORED AS TEXTFILE LOCATION &#x27;/bigdatacase/dataset&#x27;;</span><br></pre></td></tr></table></figure>
<p><strong>查询数据</strong></p>
<p>在“hive&gt;”命令提示符状态下执行下面命令查看表的信息：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; use dblab;//使用dblab数据库</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.613 seconds</span><br><span class="line">hive&gt; show tables;//显示数据库中所有表</span><br><span class="line">OK</span><br><span class="line">bigdata_user</span><br><span class="line">Time taken: 0.161 seconds, Fetched: 1 row(s)</span><br><span class="line">hive&gt; show create table bigdata_user;//查看bigdata_user表的各种属性；</span><br><span class="line">OK</span><br><span class="line">CREATE EXTERNAL TABLE `bigdata_user`(</span><br><span class="line">  `id` int, </span><br><span class="line">  `uid` string, </span><br><span class="line">  `item_id` string, </span><br><span class="line">  `behavior_type` int, </span><br><span class="line">  `item_category` string, </span><br><span class="line">  `visit_date` date, </span><br><span class="line">  `province` string)</span><br><span class="line">COMMENT &#x27;Welcome to xmudblab!&#x27;</span><br><span class="line">ROW FORMAT SERDE </span><br><span class="line">  &#x27;org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe&#x27; </span><br><span class="line">WITH SERDEPROPERTIES ( </span><br><span class="line">  &#x27;field.delim&#x27;=&#x27;\t&#x27;, </span><br><span class="line">  &#x27;serialization.format&#x27;=&#x27;\t&#x27;) </span><br><span class="line">STORED AS INPUTFORMAT </span><br><span class="line">  &#x27;org.apache.hadoop.mapred.TextInputFormat&#x27; </span><br><span class="line">OUTPUTFORMAT </span><br><span class="line">  &#x27;org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat&#x27;</span><br><span class="line">LOCATION</span><br><span class="line">  &#x27;hdfs://localhost:9000/bigdatacase/dataset&#x27;</span><br><span class="line">TBLPROPERTIES (</span><br><span class="line">  &#x27;bucketing_version&#x27;=&#x27;2&#x27;, </span><br><span class="line">  &#x27;transient_lastDdlTime&#x27;=&#x27;1718472356&#x27;)</span><br><span class="line">Time taken: 0.198 seconds, Fetched: 23 row(s)</span><br></pre></td></tr></table></figure>
<p>还可以执行下面命令查看表的简单结构：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; desc bigdata_user;</span><br><span class="line">OK</span><br><span class="line">id                      int                                         </span><br><span class="line">uid                     string                                      </span><br><span class="line">item_id                 string                                      </span><br><span class="line">behavior_type           int                                         </span><br><span class="line">item_category           string                                      </span><br><span class="line">visit_date              date                                        </span><br><span class="line">province                string                                      </span><br><span class="line">Time taken: 0.055 seconds, Fetched: 7 row(s)</span><br></pre></td></tr></table></figure>
<p>现在可以使用下面命令查询相关数据：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from bigdata_user limit 10;</span><br><span class="line">OK</span><br><span class="line">1       10001082        285259775       1       4076    2014-12-08      湖南</span><br><span class="line">2       10001082        4368907 1       5503    2014-12-12      安徽</span><br><span class="line">3       10001082        4368907 1       5503    2014-12-12      新疆</span><br><span class="line">4       10001082        53616768        1       9762    2014-12-02      上海市</span><br><span class="line">5       10001082        151466952       1       5232    2014-12-12      黑龙江</span><br><span class="line">6       10001082        53616768        4       9762    2014-12-02      广东</span><br><span class="line">7       10001082        290088061       1       5503    2014-12-12      青海</span><br><span class="line">8       10001082        298397524       1       10894   2014-12-12      香港</span><br><span class="line">9       10001082        32104252        1       6513    2014-12-12      山西</span><br><span class="line">10      10001082        323339743       1       10894   2014-12-12      宁夏</span><br><span class="line">Time taken: 1.483 seconds, Fetched: 10 row(s)</span><br><span class="line">hive&gt; select behavior_type from bigdata_user limit 10;</span><br><span class="line">OK</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">Time taken: 0.106 seconds, Fetched: 10 row(s)</span><br><span class="line">hive&gt; </span><br></pre></td></tr></table></figure>
<h2 id="hive数据分析"><a class="markdownIt-Anchor" href="#hive数据分析"></a> Hive数据分析</h2>
<h3 id="简单查询分析"><a class="markdownIt-Anchor" href="#简单查询分析"></a> 简单查询分析</h3>
<p>首先执行一条简单的指令：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select behavior_type from bigdata_user limit 10;#查看前10位用户对商品的行为</span><br><span class="line">OK</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">4</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">1</span><br><span class="line">Time taken: 0.116 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure>
<p>如果要查出每位用户购买商品时的多种信息，输出语句格式如下：<br />
select 列1，列2，….，列n from 表名；<br />
比如查询前20位用户购买商品时的时间和商品的种类，语句如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select visit_date, item_category from bigdata_user limit 20;</span><br><span class="line">OK</span><br><span class="line">2014-12-08      4076</span><br><span class="line">2014-12-12      5503</span><br><span class="line">2014-12-12      5503</span><br><span class="line">2014-12-02      9762</span><br><span class="line">2014-12-12      5232</span><br><span class="line">2014-12-02      9762</span><br><span class="line">2014-12-12      5503</span><br><span class="line">2014-12-12      10894</span><br><span class="line">2014-12-12      6513</span><br><span class="line">2014-12-12      10894</span><br><span class="line">2014-12-12      2825</span><br><span class="line">2014-11-28      2825</span><br><span class="line">2014-12-15      3200</span><br><span class="line">2014-12-03      10576</span><br><span class="line">2014-11-20      10576</span><br><span class="line">2014-12-13      10576</span><br><span class="line">2014-12-08      10576</span><br><span class="line">2014-12-14      7079</span><br><span class="line">2014-12-02      6669</span><br><span class="line">2014-12-12      5232</span><br><span class="line">Time taken: 0.089 seconds, Fetched: 20 row(s)</span><br></pre></td></tr></table></figure>
<p>有时在表中查询可以利用嵌套语句，如果列名太复杂可以设置该列的别名，以简化操作的难度，举例如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select e.bh, e.it from (select behavior_type as bh, item_category as it from bigdata_user) as e  limit 20;</span><br><span class="line">OK</span><br><span class="line">1       4076</span><br><span class="line">1       5503</span><br><span class="line">1       5503</span><br><span class="line">1       9762</span><br><span class="line">1       5232</span><br><span class="line">4       9762</span><br><span class="line">1       5503</span><br><span class="line">1       10894</span><br><span class="line">1       6513</span><br><span class="line">1       10894</span><br><span class="line">1       2825</span><br><span class="line">1       2825</span><br><span class="line">1       3200</span><br><span class="line">1       10576</span><br><span class="line">1       10576</span><br><span class="line">1       10576</span><br><span class="line">1       10576</span><br><span class="line">1       7079</span><br><span class="line">1       6669</span><br><span class="line">1       5232</span><br><span class="line">Time taken: 0.102 seconds, Fetched: 20 row(s)</span><br></pre></td></tr></table></figure>
<h3 id="查询条数统计"><a class="markdownIt-Anchor" href="#查询条数统计"></a> 查询条数统计</h3>
<p>用聚合函数count()计算出表内有多少行数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user;</span><br><span class="line">Query ID = hadoop_20240616014249_4ec3aa87-0a06-409c-9c43-40b0dd14ced7</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:42:51,137 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 01:42:52,142 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:42:57,166 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local460403743_0001</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 5169193912 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">23291027</span><br><span class="line">Time taken: 8.173 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<p>在函数内部加上distinct，查出uid不重复的数据有多少条</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(distinct uid) from bigdata_user;</span><br><span class="line">Query ID = hadoop_20240616014323_2c95c94e-e87b-4d88-9ae6-8a9ca4e8fc61</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:43:24,499 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 01:43:27,501 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:43:40,630 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local544803189_0002</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 12623097568 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">20000</span><br><span class="line">Time taken: 17.605 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<p>查询不重复的数据有多少条(为了排除客户刷单情况)</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from (select uid,item_id,behavior_type,item_category,visit_date,province from bigdata_user group by uid,item_id,behavior_type,item_category,visit_date,province       having count(*)=1) a;</span><br><span class="line">Query ID = hadoop_20240616014446_fc30d1c3-d4c1-486e-85ce-0a79b93223f2</span><br><span class="line">Total jobs = 2</span><br><span class="line">Launching Job 1 out of 2</span><br><span class="line">Number of reduce tasks not specified. Estimated from input data size: 5</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:44:48,421 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 01:45:05,693 Stage-1 map = 7%,  reduce = 0%</span><br><span class="line">2024-06-16 01:45:30,144 Stage-1 map = 18%,  reduce = 0%</span><br><span class="line">2024-06-16 01:45:32,205 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:45:44,425 Stage-1 map = 20%,  reduce = 0%</span><br><span class="line">2024-06-16 01:45:50,471 Stage-1 map = 27%,  reduce = 0%</span><br><span class="line">2024-06-16 01:46:08,032 Stage-1 map = 33%,  reduce = 0%</span><br><span class="line">2024-06-16 01:46:13,059 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:46:24,425 Stage-1 map = 40%,  reduce = 0%</span><br><span class="line">2024-06-16 01:46:30,623 Stage-1 map = 47%,  reduce = 0%</span><br><span class="line">2024-06-16 01:46:48,985 Stage-1 map = 54%,  reduce = 0%</span><br><span class="line">2024-06-16 01:46:54,057 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:47:06,424 Stage-1 map = 60%,  reduce = 0%</span><br><span class="line">2024-06-16 01:47:12,464 Stage-1 map = 67%,  reduce = 0%</span><br><span class="line">2024-06-16 01:47:30,843 Stage-1 map = 74%,  reduce = 0%</span><br><span class="line">2024-06-16 01:47:33,963 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:47:46,284 Stage-1 map = 80%,  reduce = 0%</span><br><span class="line">2024-06-16 01:47:51,291 Stage-1 map = 91%,  reduce = 0%</span><br><span class="line">2024-06-16 01:47:57,476 Stage-1 map = 95%,  reduce = 0%</span><br><span class="line">2024-06-16 01:47:59,479 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:48:11,495 Stage-1 map = 100%,  reduce = 20%</span><br><span class="line">2024-06-16 01:48:24,516 Stage-1 map = 100%,  reduce = 40%</span><br><span class="line">2024-06-16 01:48:36,535 Stage-1 map = 100%,  reduce = 60%</span><br><span class="line">2024-06-16 01:48:48,554 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">2024-06-16 01:48:49,556 Stage-1 map = 100%,  reduce = 80%</span><br><span class="line">2024-06-16 01:49:00,573 Stage-1 map = 100%,  reduce = 99%</span><br><span class="line">2024-06-16 01:49:01,578 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local924134795_0003</span><br><span class="line">Launching Job 2 out of 2</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:49:02,850 Stage-2 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local1145871970_0004</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 34984890456 HDFS Write: 0 SUCCESS</span><br><span class="line">Stage-Stage-2:  HDFS Read: 7453944616 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">22080451</span><br><span class="line">Time taken: 255.925 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<h3 id="关键字条件查询分析"><a class="markdownIt-Anchor" href="#关键字条件查询分析"></a> 关键字条件查询分析</h3>
<p><strong>以关键字的存在区间为条件的查询</strong></p>
<p>查询2014年12月10日到2014年12月13日有多少人浏览了商品。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user where behavior_type=&#x27;1&#x27; and visit_date&lt;&#x27;2014-12-13&#x27; and visit_date&gt;&#x27;2014-12-10&#x27;;</span><br><span class="line">Query ID = hadoop_20240616015218_900aa649-9b2c-4a24-8ebf-6b6a50cb5a06</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:52:19,677 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 01:52:27,914 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:52:59,420 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local2129098898_0005</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 27530904880 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">2137738</span><br><span class="line">Time taken: 41.237 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<p>以月的第n天为统计单位，依次显示第n天网站卖出去的商品的个数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(distinct uid), day(visit_date) from bigdata_user where behavior_type=&#x27;4&#x27; group by day(visit_date);</span><br><span class="line">Query ID = hadoop_20240616015346_c33511b1-be1b-4631-9e87-74152ae033e7</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks not specified. Estimated from input data size: 5</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:53:48,126 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 01:53:51,128 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:54:04,164 Stage-1 map = 100%,  reduce = 60%</span><br><span class="line">2024-06-16 01:54:05,167 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local1718417321_0006</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 59831235976 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">3098    1</span><br><span class="line">2969    8</span><br><span class="line">3110    13</span><br><span class="line">3225    16</span><br><span class="line">5267    18</span><br><span class="line">2780    21</span><br><span class="line">3104    24</span><br><span class="line">3121    4</span><br><span class="line">2804    5</span><br><span class="line">2910    11</span><br><span class="line">2922    19</span><br><span class="line">2763    22</span><br><span class="line">2990    25</span><br><span class="line">2825    26</span><br><span class="line">2828    28</span><br><span class="line">3137    3</span><br><span class="line">2957    14</span><br><span class="line">3284    15</span><br><span class="line">3049    17</span><br><span class="line">2893    23</span><br><span class="line">2867    7</span><br><span class="line">2794    9</span><br><span class="line">7604    12</span><br><span class="line">2862    20</span><br><span class="line">2913    27</span><br><span class="line">3161    2</span><br><span class="line">2850    6</span><br><span class="line">2869    10</span><br><span class="line">2710    29</span><br><span class="line">3018    30</span><br><span class="line">Time taken: 18.451 seconds, Fetched: 30 row(s)</span><br></pre></td></tr></table></figure>
<p><strong>关键字赋予给定值为条件，对其他数据进行分析</strong></p>
<p>取给定时间和给定地点，求当天发出到该地点的货物的数量。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user where province=&#x27;江西&#x27; and visit_date=&#x27;2014-12-12&#x27; and behavior_type=&#x27;4&#x27;;</span><br><span class="line">Query ID = hadoop_20240616015514_7f149dba-f8a0-4191-a9d9-399ca649580b</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:55:15,459 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 01:55:24,635 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:55:58,440 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local2110211223_0007</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 42438712192 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">885</span><br><span class="line">Time taken: 44.359 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<h3 id="根据用户行为分析"><a class="markdownIt-Anchor" href="#根据用户行为分析"></a> 根据用户行为分析</h3>
<p><strong>查询一件商品在某天的购买比例或浏览比例</strong></p>
<p>查询有多少用户在2014-12-11购买了商品</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user where visit_date=&#x27;2014-12-11&#x27;and behavior_type=&#x27;4&#x27;;</span><br><span class="line">Query ID = hadoop_20240616015657_97eeb057-f1c4-4615-8b9c-b52eecb32691</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:56:58,654 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 01:57:07,712 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:57:39,940 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local208501678_0008</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 49892615848 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">6771</span><br><span class="line">Time taken: 42.782 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<p>查询有多少用户在2014-12-11点击了该店</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user where visit_date =&#x27;2014-12-11&#x27;;</span><br><span class="line">Query ID = hadoop_20240616015820_54043f5d-8fb0-4dd9-97c8-8f2f29d8cc1f</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 01:58:22,332 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 01:58:30,479 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 01:59:01,726 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local981603111_0009</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 57346519504 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">944979</span><br><span class="line">Time taken: 40.809 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<p><strong>查询某个用户在某一天点击网站占该天所有点击行为的比例（点击行为包括浏览、加入购物车、收藏、购买）</strong></p>
<p>查询用户10001082在2014-12-12点击网站的次数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user where uid=10001082 and visit_date=&#x27;2014-12-12&#x27;;</span><br><span class="line">Query ID = hadoop_20240616020035_9ad77af3-2cba-4456-924d-9646ed7be7d0</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 02:00:36,927 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 02:00:46,135 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 02:01:20,744 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local219554971_0010</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 64800423160 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">69</span><br><span class="line">Time taken: 45.22 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<p>查询所有用户在这一天点击该网站的次数</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select count(*) from bigdata_user where visit_date=&#x27;2014-12-12&#x27;;</span><br><span class="line">Query ID = hadoop_20240616020246_fcb99cc6-d131-4e1c-9cc0-ce38b7de499d</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 02:02:48,411 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 02:02:56,581 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 02:03:28,882 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local1580275434_0011</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 72254326816 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">1344980</span><br><span class="line">Time taken: 41.911 seconds, Fetched: 1 row(s)</span><br></pre></td></tr></table></figure>
<p><strong>给定购买商品的数量范围，查询某一天在该网站的购买该数量商品的用户id</strong></p>
<p>查询某一天在该网站购买商品超过5次的用户id</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select uid from bigdata_user where behavior_type=&#x27;4&#x27; and visit_date=&#x27;2014-12-12&#x27; group by uid having count(behavior_type=&#x27;4&#x27;)&gt;5;</span><br><span class="line">Query ID = hadoop_20240616020903_0ad7cef8-7e49-4107-8683-fd98599a18e1</span><br><span class="line">Total jobs = 1</span><br><span class="line">Launching Job 1 out of 1</span><br><span class="line">Number of reduce tasks not specified. Estimated from input data size: 5</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 02:09:07,206 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 02:09:18,314 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 02:09:56,917 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local1592485813_0001</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 10138340136 HDFS Write: 0 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">100605</span><br><span class="line">100728355</span><br><span class="line">100796503</span><br><span class="line">10095384</span><br><span class="line">...</span><br><span class="line">64315314</span><br><span class="line">6500849</span><br><span class="line">65095287</span><br><span class="line">6534626</span><br><span class="line">Time taken: 53.599 seconds, Fetched: 1564 row(s)</span><br></pre></td></tr></table></figure>
<h3 id="用户实时查询分析"><a class="markdownIt-Anchor" href="#用户实时查询分析"></a> 用户实时查询分析</h3>
<p><strong>查询某个地区的用户当天浏览网站的次数</strong></p>
<p>创建新的数据表进行存储</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table scan(province STRING,scan INT) COMMENT &#x27;This is the search of bigdataday&#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 1.17 seconds</span><br></pre></td></tr></table></figure>
<p>导入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; insert overwrite table scan select province,count(behavior_type) from bigdata_user where behavior_type=&#x27;1&#x27; group by province;</span><br><span class="line">Query ID = hadoop_20240616021413_4ab776e4-cc98-475c-95bc-6973ee82e279</span><br><span class="line">Total jobs = 2</span><br><span class="line">Launching Job 1 out of 2</span><br><span class="line">Number of reduce tasks not specified. Estimated from input data size: 5</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 02:14:15,184 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 02:14:17,188 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 02:14:27,290 Stage-1 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local52434044_0002</span><br><span class="line">Loading data to table dblab.scan</span><br><span class="line">Launching Job 2 out of 2</span><br><span class="line">Number of reduce tasks determined at compile time: 1</span><br><span class="line">In order to change the average load for a reducer (in bytes):</span><br><span class="line">  set hive.exec.reducers.bytes.per.reducer=&lt;number&gt;</span><br><span class="line">In order to limit the maximum number of reducers:</span><br><span class="line">  set hive.exec.reducers.max=&lt;number&gt;</span><br><span class="line">In order to set a constant number of reducers:</span><br><span class="line">  set mapreduce.job.reduces=&lt;number&gt;</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 02:14:28,626 Stage-3 map = 100%,  reduce = 100%</span><br><span class="line">Ended Job = job_local2032099341_0003</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 22561512896 HDFS Write: 2644 SUCCESS</span><br><span class="line">Stage-Stage-3:  HDFS Read: 4969269104 HDFS Write: 1654 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">Time taken: 15.49 seconds</span><br></pre></td></tr></table></figure>
<p>显示结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from scan;</span><br><span class="line">OK</span><br><span class="line">台湾    645421</span><br><span class="line">四川    646636</span><br><span class="line">宁夏    643964</span><br><span class="line">安徽    645594</span><br><span class="line">广东    643067</span><br><span class="line">广西    645380</span><br><span class="line">河南    645191</span><br><span class="line">贵州    645203</span><br><span class="line">辽宁    644007</span><br><span class="line">陕西    645452</span><br><span class="line">吉林    645223</span><br><span class="line">江西    644830</span><br><span class="line">甘肃    645495</span><br><span class="line">西藏    644691</span><br><span class="line">重庆市  645230</span><br><span class="line">上海市  644384</span><br><span class="line">云南    645295</span><br><span class="line">北京市  645735</span><br><span class="line">江苏    646701</span><br><span class="line">浙江    645915</span><br><span class="line">海南    646311</span><br><span class="line">湖北    645080</span><br><span class="line">青海    645554</span><br><span class="line">内蒙古  645438</span><br><span class="line">天津市  645704</span><br><span class="line">山东    646162</span><br><span class="line">山西    644977</span><br><span class="line">新疆    644826</span><br><span class="line">湖南    645725</span><br><span class="line">澳门    645513</span><br><span class="line">香港    644758</span><br><span class="line">黑龙江  645571</span><br><span class="line">河北    645313</span><br><span class="line">福建    646174</span><br><span class="line">Time taken: 0.093 seconds, Fetched: 34 row(s)</span><br></pre></td></tr></table></figure>
<h2 id="hive-mysql-hbase数据互导"><a class="markdownIt-Anchor" href="#hive-mysql-hbase数据互导"></a> Hive、MySQL、HBase数据互导</h2>
<h3 id="hive预操作"><a class="markdownIt-Anchor" href="#hive预操作"></a> Hive预操作</h3>
<p>创建临时表<code>user_action</code></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; create table dblab.user_action(id STRING,uid STRING, item_id STRING, behavior_type STRING, item_category STRING, visit_date DATE, province STRING) COMMENT &#x27;Welcome to XMU dblab! &#x27; ROW FORMAT DELIMITED FIELDS TERMINATED BY &#x27;\t&#x27; STORED AS TEXTFILE;</span><br><span class="line">OK</span><br><span class="line">Time taken: 0.277 seconds</span><br></pre></td></tr></table></figure>
<p>现在可以新建一个终端，执行命令查看一下，确认这个数据文件在HDFS中确实已经被创建，在新建的终端中执行下面命令</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -<span class="built_in">ls</span> /user/hive/warehouse/dblab.db/</span><br></pre></td></tr></table></figure>
<p>可以看到目录内容：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Found 2 items</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2024-06-16 02:14 /user/hive/warehouse/dblab.db/scan</span><br><span class="line">drwxr-xr-x   - hadoop supergroup          0 2024-06-16 02:18 /user/hive/warehouse/dblab.db/user_action</span><br></pre></td></tr></table></figure>
<p><strong>将<code>bigdata_user</code>表中的数据插入到<code>user_action</code></strong></p>
<p>下面把<code>dblab.bigdata_user</code>数据插入到<code>dblab.user_action</code>表中，命令如下：</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; INSERT OVERWRITE TABLE dblab.user_action select * from dblab.bigdata_user;</span><br><span class="line">Query ID = hadoop_20240616022349_4259971c-f495-4711-be42-69c01bc36c59</span><br><span class="line">Total jobs = 3</span><br><span class="line">Launching Job 1 out of 3</span><br><span class="line">Number of reduce tasks is set to 0 since there&#x27;s no reduce operator</span><br><span class="line">Job running in-process (local Hadoop)</span><br><span class="line">2024-06-16 02:23:51,241 Stage-1 map = 0%,  reduce = 0%</span><br><span class="line">2024-06-16 02:24:03,244 Stage-1 map = 10%,  reduce = 0%</span><br><span class="line">2024-06-16 02:24:06,246 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 02:24:18,254 Stage-1 map = 30%,  reduce = 0%</span><br><span class="line">2024-06-16 02:24:24,260 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 02:24:36,268 Stage-1 map = 50%,  reduce = 0%</span><br><span class="line">2024-06-16 02:24:41,272 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">2024-06-16 02:24:53,281 Stage-1 map = 70%,  reduce = 0%</span><br><span class="line">2024-06-16 02:24:58,285 Stage-1 map = 100%,  reduce = 0%</span><br><span class="line">Ended Job = job_local1890104177_0013</span><br><span class="line">Stage-4 is selected by condition resolver.</span><br><span class="line">Stage-3 is filtered out by condition resolver.</span><br><span class="line">Stage-5 is filtered out by condition resolver.</span><br><span class="line">Moving data to directory hdfs://localhost:9000/user/hive/warehouse/dblab.db/user_action/.hive-staging_hive_2024-06-16_02-23-49_848_6247200484593972572-1/-ext-10000</span><br><span class="line">Loading data to table dblab.user_action</span><br><span class="line">MapReduce Jobs Launched: </span><br><span class="line">Stage-Stage-1:  HDFS Read: 72254308806 HDFS Write: 3926636398 SUCCESS</span><br><span class="line">Total MapReduce CPU Time Spent: 0 msec</span><br><span class="line">OK</span><br><span class="line">Time taken: 79.633 seconds</span><br></pre></td></tr></table></figure>
<p>然后执行下面命令查询上面的插入命令是否成功执行</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">hive&gt; select * from user_action limit 10;</span><br><span class="line">OK</span><br><span class="line">1       10001082        285259775       1       4076    2014-12-08      湖南</span><br><span class="line">2       10001082        4368907 1       5503    2014-12-12      安徽</span><br><span class="line">3       10001082        4368907 1       5503    2014-12-12      新疆</span><br><span class="line">4       10001082        53616768        1       9762    2014-12-02      上海市</span><br><span class="line">5       10001082        151466952       1       5232    2014-12-12      黑龙江</span><br><span class="line">6       10001082        53616768        4       9762    2014-12-02      广东</span><br><span class="line">7       10001082        290088061       1       5503    2014-12-12      青海</span><br><span class="line">8       10001082        298397524       1       10894   2014-12-12      香港</span><br><span class="line">9       10001082        32104252        1       6513    2014-12-12      山西</span><br><span class="line">10      10001082        323339743       1       10894   2014-12-12      宁夏</span><br><span class="line">Time taken: 0.081 seconds, Fetched: 10 row(s)</span><br></pre></td></tr></table></figure>
<h3 id="使用java-api将数据从hive导入mysql"><a class="markdownIt-Anchor" href="#使用java-api将数据从hive导入mysql"></a> 使用Java API将数据从Hive导入MySQL</h3>
<p><strong>将前面生成的临时表数据从Hive导入到 MySQL中</strong></p>
<p>登录MySQL</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> mysql -u root</span><br></pre></td></tr></table></figure>
<p>创建数据库并授权给<code>hadoop</code>用户</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases; # 显示所有数据库</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> hive               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> mysql              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> sys                <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">create</span> database dblab; 创建dblab数据库</span><br><span class="line">Query OK, <span class="number">1</span> <span class="type">row</span> affected (<span class="number">0.02</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">grant</span> <span class="keyword">all</span> privileges <span class="keyword">on</span> dblab.<span class="operator">*</span> <span class="keyword">to</span> hadoop; # 将dblab数据库授权给hadoop用户</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure>
<p>退出<code>root</code>用户</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> exit;</span><br><span class="line">Bye</span><br></pre></td></tr></table></figure>
<p>登陆<code>hadoop</code>用户</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> databases; # 显示所有数据库</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> Database           <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="operator">|</span> dblab              <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> hive               <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> information_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> performance_schema <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------+</span></span><br><span class="line"><span class="number">4</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.00</span> sec)</span><br><span class="line"></span><br><span class="line">mysql<span class="operator">&gt;</span> use dblab;</span><br><span class="line">Database changed</span><br></pre></td></tr></table></figure>
<p>注意：用下面命令查看数据库的编码</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">show</span> variables <span class="keyword">like</span> &quot;char%&quot;;</span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+----------------------------+</span></span><br><span class="line"><span class="operator">|</span> Variable_name            <span class="operator">|</span> <span class="keyword">Value</span>                      <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+----------------------------+</span></span><br><span class="line"><span class="operator">|</span> character_set_client     <span class="operator">|</span> utf8mb4                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> character_set_connection <span class="operator">|</span> utf8mb4                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> character_set_database   <span class="operator">|</span> utf8mb4                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> character_set_filesystem <span class="operator">|</span> <span class="type">binary</span>                     <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> character_set_results    <span class="operator">|</span> utf8mb4                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> character_set_server     <span class="operator">|</span> utf8mb4                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> character_set_system     <span class="operator">|</span> utf8mb3                    <span class="operator">|</span></span><br><span class="line"><span class="operator">|</span> character_sets_dir       <span class="operator">|</span> <span class="operator">/</span>usr<span class="operator">/</span>share<span class="operator">/</span>mysql<span class="operator">/</span>charsets<span class="operator">/</span> <span class="operator">|</span></span><br><span class="line"><span class="operator">+</span><span class="comment">--------------------------+----------------------------+</span></span><br><span class="line"><span class="number">8</span> <span class="keyword">rows</span> <span class="keyword">in</span> <span class="keyword">set</span> (<span class="number">0.01</span> sec)</span><br></pre></td></tr></table></figure>
<blockquote>
<p>如果character_set_server不是utf8之类的编码，则需要修改配置文件</p>
<p>在<code>[mysqld]</code>下添加一行<code>character_set_server=utf8</code></p>
<figure class="highlight plaintext"><figcaption><span>/etc/mysql/mysql.conf.d/mysqld.cnf</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">[mysqld]</span><br><span class="line">character_set_server=utf8</span><br></pre></td></tr></table></figure>
<p>然后重启MySQL服务</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> service mysql restart</span><br></pre></td></tr></table></figure>
</blockquote>
<p>创建表</p>
<p>下面在MySQL的数据库<code>dblab</code>中创建一个新表<code>user_action</code>，并设置其编码为<code>utf-8</code>：</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">mysql<span class="operator">&gt;</span> <span class="keyword">CREATE</span> <span class="keyword">TABLE</span> `dblab`.`user_action` (`id` <span class="type">varchar</span>(<span class="number">50</span>),`uid` <span class="type">varchar</span>(<span class="number">50</span>),`item_id` <span class="type">varchar</span>(<span class="number">50</span>),`behavior_type` <span class="type">varchar</span>(<span class="number">10</span>),`item_category` <span class="type">varchar</span>(<span class="number">50</span>), `visit_date` <span class="type">DATE</span>,`province` <span class="type">varchar</span>(<span class="number">20</span>)) ENGINE<span class="operator">=</span>InnoDB <span class="keyword">DEFAULT</span> CHARSET<span class="operator">=</span>utf8;</span><br><span class="line">Query OK, <span class="number">0</span> <span class="keyword">rows</span> affected, <span class="number">1</span> warning (<span class="number">0.03</span> sec)</span><br></pre></td></tr></table></figure>
<p>创建成功后<code>exit</code>退出MySQL</p>
<p>导入数据</p>
<p>通过JDBC连接Hive和MySQL，将数据从Hive导入MySQL。通过JDBC连接Hive，需要通过Hive的thrift服务实现跨语言访问Hive，实现thrift服务需要开启hiveserver2。</p>
<p>启动HDFS以后，在终端中启动<code>metastore</code></p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service metastore</span><br></pre></td></tr></table></figure>
<p>在另一个终端中执行以下命令开启<code>HiveServer2</code>，并设置默认端口为10000（如果按照前面教程配置了hiveserver2的端口，这里的<code>-hiveconf</code>选项不需要添加）</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive --service hiveserver2 -hiveconf hive.server2.thrift.port=10000</span><br></pre></td></tr></table></figure>
<p>启动时，当屏幕上出现“Hive Session ID = 6bd1726e-37c5-41fc-93ea-ef7e176b24f2”信息时，会停留较长的时间，需要出现几个“Hive Session ID=…”以后，Hive才会真正启动。启动成功以后，会出现如下信息</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Hive Session ID = bb1c1106-83ce-4653-9541-c79a86945ea0</span><br><span class="line">Hive Session ID = 0e577e34-83c1-40de-9fb6-917be3d5e022</span><br></pre></td></tr></table></figure>
<p>不要关闭这个终端，保持服务的运行</p>
<p>在新的终端下运行以下命令查看10000号和10002号端口是否已经被占用，如果被占用，说明启动成功，可以在浏览器访问<code>127.0.0.1:10002</code>打开hiveserver2的网页接口</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">sudo</span> netstat -anp |grep <span class="string">&quot;10000\|10002&quot;</span></span><br></pre></td></tr></table></figure>
<p>端口信息如下</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">tcp6       0      0 :::10000                :::*                    LISTEN      76077/java          </span><br><span class="line">tcp6       0      0 :::10002                :::*                    LISTEN      76077/java</span><br></pre></td></tr></table></figure>
<p>启动IntelliJ IDEA，创建一个项目名为HivetoMySQL</p>
<p>将<code>$HADOOP_HOME/share/hadoop/common</code>和<code>$HIVE_HOME/lib</code>目录下JAR包导入到项目中，<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup> <a id="jar"></a></p>
<ol>
<li>点击工具栏的 File 选项</li>
<li>点击 Project Structure（CTRL + SHIFT + ALT + S on Windows/Linux，⌘ + ; on Mac OS X）</li>
<li>选择左侧的 Modules 选项</li>
<li>选择 Dependencies tab</li>
<li>点击 + → JARs or directories</li>
<li>选择需要导入JAR包的所在目录</li>
<li>点击 Apply 应用选项</li>
</ol>
<p>导入后可以在左边的External Libraries中看到导入的JAR包</p>
<p>删除原有的<code>Main</code>类，创建一个公共类<code>HivetoMySQL</code></p>
<p>编写<code>HivetoMySQL.java</code>代码，注意用户名和密码，以及端口是否正确，运行代码等待导入</p>
<p>然后就是漫漫漫漫漫漫漫漫漫漫漫漫长长长长长长长长长长长长长长的等待时间…（人生苦短，不要用raw_user…）</p>
<figure class="highlight java"><figcaption><span>HivetoMySQL</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.sql.*;</span><br><span class="line"><span class="keyword">import</span> java.sql.SQLException;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">HivetoMySQL</span> &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">String</span> <span class="variable">driverName</span> <span class="operator">=</span> <span class="string">&quot;org.apache.hive.jdbc.HiveDriver&quot;</span>;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="type">String</span> <span class="variable">driverName_mysql</span> <span class="operator">=</span> <span class="string">&quot;com.mysql.jdbc.Driver&quot;</span>;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> SQLException &#123;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            Class.forName(driverName);</span><br><span class="line">        &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">            <span class="comment">// TODO Auto-generated catch block</span></span><br><span class="line">            e.printStackTrace();</span><br><span class="line">            System.exit(<span class="number">1</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">Connection</span> <span class="variable">con1</span> <span class="operator">=</span> DriverManager.getConnection(<span class="string">&quot;jdbc:hive2://localhost:10000/default&quot;</span>, <span class="string">&quot;hive&quot;</span>, <span class="string">&quot;hive&quot;</span>);<span class="comment">//后两个参数是用户名密码</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (con1 == <span class="literal">null</span>) System.out.println(<span class="string">&quot;连接失败&quot;</span>);</span><br><span class="line">        <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="type">Statement</span> <span class="variable">stmt</span> <span class="operator">=</span> con1.createStatement();</span><br><span class="line">            <span class="type">String</span> <span class="variable">sql</span> <span class="operator">=</span> <span class="string">&quot;select * from dblab.user_action&quot;</span>;</span><br><span class="line">            System.out.println(<span class="string">&quot;Running: &quot;</span> + sql);</span><br><span class="line">            <span class="type">ResultSet</span> <span class="variable">res</span> <span class="operator">=</span> stmt.executeQuery(sql);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//InsertToMysql</span></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                Class.forName(driverName_mysql);</span><br><span class="line">                <span class="type">Connection</span> <span class="variable">con2</span> <span class="operator">=</span> DriverManager.getConnection(<span class="string">&quot;jdbc:mysql://localhost:3306/dblab&quot;</span>, <span class="string">&quot;hadoop&quot;</span>, <span class="string">&quot;hadoop&quot;</span>);</span><br><span class="line">                <span class="type">String</span> <span class="variable">sql2</span> <span class="operator">=</span> <span class="string">&quot;insert into user_action(id,uid,item_id,behavior_type,item_category,visit_date,province) values (?,?,?,?,?,?,?)&quot;</span>;</span><br><span class="line">                <span class="type">PreparedStatement</span> <span class="variable">ps</span> <span class="operator">=</span> con2.prepareStatement(sql2);</span><br><span class="line">                <span class="keyword">while</span> (res.next()) &#123;</span><br><span class="line">                    ps.setString(<span class="number">1</span>, res.getString(<span class="number">1</span>));</span><br><span class="line">                    ps.setString(<span class="number">2</span>, res.getString(<span class="number">2</span>));</span><br><span class="line">                    ps.setString(<span class="number">3</span>, res.getString(<span class="number">3</span>));</span><br><span class="line">                    ps.setString(<span class="number">4</span>, res.getString(<span class="number">4</span>));</span><br><span class="line">                    ps.setString(<span class="number">5</span>, res.getString(<span class="number">5</span>));</span><br><span class="line">                    ps.setDate(<span class="number">6</span>, res.getDate(<span class="number">6</span>));</span><br><span class="line">                    ps.setString(<span class="number">7</span>, res.getString(<span class="number">7</span>));</span><br><span class="line">                    ps.executeUpdate();</span><br><span class="line">                &#125;</span><br><span class="line">                ps.close();</span><br><span class="line">                con2.close();</span><br><span class="line">                res.close();</span><br><span class="line">                stmt.close();</span><br><span class="line">            &#125; <span class="keyword">catch</span> (ClassNotFoundException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        con1.close();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="使用hbase-java-api把数据从本地导入到hbase中"><a class="markdownIt-Anchor" href="#使用hbase-java-api把数据从本地导入到hbase中"></a> 使用HBase Java API把数据从本地导入到HBase中</h3>
<p><strong>数据准备</strong></p>
<p>先将之前的<code>user_action</code>数据从HDFS复制到Linux系统的本地文件系统中</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hdfs dfs -get /user/hive/warehouse/dblab.db/user_action /usr/local/bigdatacase/dataset/</span><br><span class="line"><span class="built_in">cat</span> /usr/local/bigdatacase/dataset/user_action/* | <span class="built_in">head</span> -10     <span class="comment"># 查看前十行数据</span></span><br><span class="line"><span class="built_in">cat</span> /usr/local/bigdatacase/dataset/user_action/00000* &gt; /usr/local/bigdatacase/dataset/user_action.output    <span class="comment">#将00000*数据复制一份命名为`user_action.output`</span></span><br><span class="line"><span class="built_in">head</span> /usr/local/bigdatacase/dataset/user_action.output      <span class="comment"># 查看user_action.output的前十行</span></span><br></pre></td></tr></table></figure>
<p>创建一个项目，起名叫<code>ImportHBase</code>，像<a href="#jar">上面</a>一样将目录<code>$HADOOP_HOME/share/hadoop/common</code>和<code>$HBASE_HOME/lib</code>下的JAR包导入到项目中，删除原有的<code>Main</code>类，新建一个<code>ImportHBase</code>类</p>
<p>编写代码<code>ImportHBase.java</code></p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> java.io.BufferedReader;</span><br><span class="line"><span class="keyword">import</span> java.io.FileInputStream;</span><br><span class="line"><span class="keyword">import</span> java.io.IOException;</span><br><span class="line"><span class="keyword">import</span> java.io.InputStreamReader;</span><br><span class="line"><span class="keyword">import</span> java.util.List;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.conf.Configuration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.HBaseConfiguration;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.client.*;</span><br><span class="line"><span class="keyword">import</span> org.apache.hadoop.hbase.util.Bytes;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">class</span> <span class="title class_">ImportHBase</span> <span class="keyword">extends</span> <span class="title class_">Thread</span> &#123;</span><br><span class="line">    <span class="keyword">public</span> Configuration config;</span><br><span class="line">    <span class="keyword">public</span> Connection conn;</span><br><span class="line">    <span class="keyword">public</span> Table table;</span><br><span class="line">    <span class="keyword">public</span> Admin admin;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="title function_">ImportHBase</span><span class="params">()</span> &#123;</span><br><span class="line">        config = HBaseConfiguration.create();</span><br><span class="line"><span class="comment">//      config.set(&quot;hbase.master&quot;, &quot;master:60000&quot;);</span></span><br><span class="line"><span class="comment">//      config.set(&quot;hbase.zookeeper.quorum&quot;, &quot;master&quot;);</span></span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            conn = ConnectionFactory.createConnection(config);</span><br><span class="line">            admin = conn.getAdmin();</span><br><span class="line">            table = conn.getTable(TableName.valueOf(<span class="string">&quot;user_action&quot;</span>));</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title function_">main</span><span class="params">(String[] args)</span> <span class="keyword">throws</span> Exception &#123;</span><br><span class="line">        <span class="keyword">if</span> (args.length == <span class="number">0</span>) &#123;       <span class="comment">//第一个参数是该jar所使用的类，第二个参数是数据集所存放的路径</span></span><br><span class="line">            <span class="keyword">throw</span> <span class="keyword">new</span> <span class="title class_">Exception</span>(<span class="string">&quot;You must set input path!&quot;</span>);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">String</span> <span class="variable">fileName</span> <span class="operator">=</span> args[args.length - <span class="number">1</span>];  <span class="comment">//输入的文件路径是最后一个参数</span></span><br><span class="line">        <span class="type">ImportHBase</span> <span class="variable">test</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">ImportHBase</span>();</span><br><span class="line">        test.importLocalFileToHBase(fileName);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">importLocalFileToHBase</span><span class="params">(String fileName)</span> &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">st</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        <span class="type">BufferedReader</span> <span class="variable">br</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">        <span class="keyword">try</span> &#123;</span><br><span class="line">            br = <span class="keyword">new</span> <span class="title class_">BufferedReader</span>(<span class="keyword">new</span> <span class="title class_">InputStreamReader</span>(<span class="keyword">new</span> <span class="title class_">FileInputStream</span>(</span><br><span class="line">                    fileName)));</span><br><span class="line">            <span class="type">String</span> <span class="variable">line</span> <span class="operator">=</span> <span class="literal">null</span>;</span><br><span class="line">            <span class="type">int</span> <span class="variable">count</span> <span class="operator">=</span> <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">while</span> ((line = br.readLine()) != <span class="literal">null</span>) &#123;</span><br><span class="line">                count++;</span><br><span class="line">                put(line);</span><br><span class="line">                <span class="keyword">if</span> (count % <span class="number">10000</span> == <span class="number">0</span>)</span><br><span class="line">                    System.out.println(count);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">            e.printStackTrace();</span><br><span class="line">        &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">            <span class="keyword">if</span> (br != <span class="literal">null</span>) &#123;</span><br><span class="line">                <span class="keyword">try</span> &#123;</span><br><span class="line">                    br.close();</span><br><span class="line">                &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                    e.printStackTrace();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">                table.close(); <span class="comment">// must close the client</span></span><br><span class="line">            &#125; <span class="keyword">catch</span> (IOException e) &#123;</span><br><span class="line">                e.printStackTrace();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="type">long</span> <span class="variable">en2</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        System.out.println(<span class="string">&quot;Total Time: &quot;</span> + (en2 - st) + <span class="string">&quot; ms&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@SuppressWarnings(&quot;deprecation&quot;)</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">put</span><span class="params">(String line)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        String[] arr = line.split(<span class="string">&quot;\t&quot;</span>, -<span class="number">1</span>);</span><br><span class="line">        String[] column = &#123;<span class="string">&quot;id&quot;</span>, <span class="string">&quot;uid&quot;</span>, <span class="string">&quot;item_id&quot;</span>, <span class="string">&quot;behavior_type&quot;</span>, <span class="string">&quot;item_category&quot;</span>, <span class="string">&quot;date&quot;</span>, <span class="string">&quot;province&quot;</span>&#125;;</span><br><span class="line">        <span class="keyword">if</span> (arr.length == <span class="number">7</span>) &#123;</span><br><span class="line">            <span class="type">Put</span> <span class="variable">put</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Put</span>(Bytes.toBytes(arr[<span class="number">0</span>]));<span class="comment">// rowkey</span></span><br><span class="line">            <span class="keyword">for</span> (<span class="type">int</span> <span class="variable">i</span> <span class="operator">=</span> <span class="number">1</span>; i &lt; arr.length; i++) &#123;</span><br><span class="line">                put.addColumn(Bytes.toBytes(<span class="string">&quot;f1&quot;</span>), Bytes.toBytes(column[i]), Bytes.toBytes(arr[i]));</span><br><span class="line">            &#125;</span><br><span class="line">            table.put(put); <span class="comment">// put to server</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">void</span> <span class="title function_">get</span><span class="params">(String rowkey, String columnFamily, String column,</span></span><br><span class="line"><span class="params">                    <span class="type">int</span> versions)</span> <span class="keyword">throws</span> IOException &#123;</span><br><span class="line">        <span class="type">long</span> <span class="variable">st</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        <span class="type">Get</span> <span class="variable">get</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Get</span>(Bytes.toBytes(rowkey));</span><br><span class="line">        get.addColumn(Bytes.toBytes(columnFamily), Bytes.toBytes(column));</span><br><span class="line">        <span class="type">Scan</span> <span class="variable">scanner</span> <span class="operator">=</span> <span class="keyword">new</span> <span class="title class_">Scan</span>(get);</span><br><span class="line">        scanner.readVersions(versions);</span><br><span class="line">        <span class="type">ResultScanner</span> <span class="variable">rsScanner</span> <span class="operator">=</span> table.getScanner(scanner);</span><br><span class="line">        <span class="keyword">for</span> (Result result : rsScanner) &#123;</span><br><span class="line">            <span class="keyword">final</span> List&lt;Cell&gt; list = result.listCells();</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">final</span> Cell kv : list) &#123;</span><br><span class="line">                System.out.println(Bytes.toStringBinary(kv.getValueArray()) + <span class="string">&quot;\t&quot;</span></span><br><span class="line">                        + kv.getTimestamp()); <span class="comment">// mid + time</span></span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">        rsScanner.close();</span><br><span class="line">        <span class="type">long</span> <span class="variable">en2</span> <span class="operator">=</span> System.currentTimeMillis();</span><br><span class="line">        System.out.println(<span class="string">&quot;Total Time: &quot;</span> + (en2 - st) + <span class="string">&quot; ms&quot;</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>打包成可执行jar包<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup></p>
<ol>
<li>点击工具栏 Files 选项</li>
<li>点击 Project Structure（CTRL + SHIFT + ALT + S on Windows/Linux，⌘ + ; on Mac OS X）</li>
<li>点击 Artifacts</li>
<li>点击 + 号，选择JAR，From Modules with dependencies</li>
<li>Main Class 选择ImportHBase</li>
<li>JAR files from libraries 选择 extract to the target JAR</li>
<li>选择META-INF/MANIFEST.MF的路径，路径选择src即可</li>
<li>点击OK完成配置</li>
<li>点击工具栏 Build 选项</li>
<li>点击Build Artifact</li>
<li>点击 Build 等待编译</li>
</ol>
<p>将生成的jar包复制到<code>/usr/local/bigdatacase/hbase</code>目录</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">mkdir</span> /usr/local/bigdatacase/hbase</span><br><span class="line"><span class="built_in">cp</span> ImportHBase.jar /usr/local/bigdatacase/hbase</span><br></pre></td></tr></table></figure>
<p>启动HDFS、ZooKeeper和HBase</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">start-dfs.sh</span><br><span class="line">zkServer.sh start</span><br><span class="line">start-hbase.sh</span><br></pre></td></tr></table></figure>
<p>启动HBase Shell</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase shell</span><br></pre></td></tr></table></figure>
<p>启动成功后，进入&quot;hbase&gt;&quot;命令提示符状态</p>
<p>创建<code>user_action</code>表</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase:001:0&gt; create &#x27;user_action&#x27;, &#123; NAME =&gt; &#x27;f1&#x27;, VERSIONS =&gt; 5&#125;</span><br><span class="line">Created table user_action</span><br><span class="line">Took 1.5141 seconds                                                                                                                     </span><br><span class="line">=&gt; Hbase::Table - user_action</span><br></pre></td></tr></table></figure>
<p>确保<code>user_action</code>表为空</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">hbase:001:0&gt; truncate &#x27;user_action&#x27;</span><br><span class="line">Truncating &#x27;user_action&#x27; table (it may take a while):</span><br><span class="line">Disabling table...</span><br><span class="line">Truncating table...</span><br><span class="line">Took 2.9350 seconds</span><br></pre></td></tr></table></figure>
<p>通过hadoop jar命令运行上面的Java程序</p>
<figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop jar /usr/local/bigdatacase/hbase/ImportHBase ImportHBase /usr/local/bigdatacase/dataset/user_action.output</span><br></pre></td></tr></table></figure>
<p>然后就是漫长的等待时间（相较于上面的Hive导入MySQL，已经是非常非常快了）</p>
<p>导入完成以后在HBase Shell窗口，执行下面命令查询数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">hbase:001:0&gt; scan &#x27;user_action&#x27;,&#123;LIMIT=&gt;10&#125; </span><br><span class="line">ROW                                                                   COLUMN+CELL                                                                                                                                                                                                  </span><br><span class="line"> 1                                                                    column=f1:behavior_type, timestamp=2024-06-16T04:51:32.554, value=1                                                                                                                                          </span><br><span class="line"> 1                                                                    column=f1:date, timestamp=2024-06-16T04:51:32.554, value=2014-12-08                                                                                                                                          </span><br><span class="line"> 1                                                                    column=f1:item_category, timestamp=2024-06-16T04:51:32.554, value=4076                                                                                                                                       </span><br><span class="line"> 1                                                                    column=f1:item_id, timestamp=2024-06-16T04:51:32.554, value=285259775                                                                                                                                        </span><br><span class="line"> 1                                                                    column=f1:province, timestamp=2024-06-16T04:51:32.554, value=\xE6\xB9\x96\xE5\x8D\x97                                                                                                                        </span><br><span class="line"> 1                                                                    column=f1:uid, timestamp=2024-06-16T04:51:32.554, value=10001082                                                                                                                                             </span><br><span class="line"> 10                                                                   column=f1:behavior_type, timestamp=2024-06-16T04:51:32.573, value=1                                                                                                                                          </span><br><span class="line"> 10                                                                   column=f1:date, timestamp=2024-06-16T04:51:32.573, value=2014-12-12                                                                                                                                          </span><br><span class="line"> 10                                                                   column=f1:item_category, timestamp=2024-06-16T04:51:32.573, value=10894                                                                                                                                      </span><br><span class="line"> 10                                                                   column=f1:item_id, timestamp=2024-06-16T04:51:32.573, value=323339743                                                                                                                                        </span><br><span class="line"> 10                                                                   column=f1:province, timestamp=2024-06-16T04:51:32.573, value=\xE5\xAE\x81\xE5\xA4\x8F                                                                                                                        </span><br><span class="line"> 10                                                                   column=f1:uid, timestamp=2024-06-16T04:51:32.573, value=10001082                                                                                                                                             </span><br><span class="line"> 100                                                                  column=f1:behavior_type, timestamp=2024-06-16T04:51:32.709, value=1                                                                                                                                          </span><br><span class="line"> 100                                                                  column=f1:date, timestamp=2024-06-16T04:51:32.709, value=2014-12-02                                                                                                                                          </span><br><span class="line"> 100                                                                  column=f1:item_category, timestamp=2024-06-16T04:51:32.709, value=10576                                                                                                                                      </span><br><span class="line"> 100                                                                  column=f1:item_id, timestamp=2024-06-16T04:51:32.709, value=275221686                                                                                                                                        </span><br><span class="line"> 100                                                                  column=f1:province, timestamp=2024-06-16T04:51:32.709, value=\xE6\xB1\x9F\xE8\x8B\x8F                                                                                                                        </span><br><span class="line"> 100                                                                  column=f1:uid, timestamp=2024-06-16T04:51:32.709, value=10001082                                                                                                                                             </span><br><span class="line"> 1000                                                                 column=f1:behavior_type, timestamp=2024-06-16T04:51:33.352, value=1                                                                                                                                          </span><br><span class="line"> 1000                                                                 column=f1:date, timestamp=2024-06-16T04:51:33.352, value=2014-12-02                                                                                                                                          </span><br><span class="line"> 1000                                                                 column=f1:item_category, timestamp=2024-06-16T04:51:33.352, value=3381                                                                                                                                       </span><br><span class="line"> 1000                                                                 column=f1:item_id, timestamp=2024-06-16T04:51:33.352, value=168463559                                                                                                                                        </span><br><span class="line"> 1000                                                                 column=f1:province, timestamp=2024-06-16T04:51:33.352, value=\xE5\x86\x85\xE8\x92\x99\xE5\x8F\xA4                                                                                                            </span><br><span class="line"> 1000                                                                 column=f1:uid, timestamp=2024-06-16T04:51:33.352, value=100068031                                                                                                                                            </span><br><span class="line"> 10000                                                                column=f1:behavior_type, timestamp=2024-06-16T04:51:35.031, value=1                                                                                                                                          </span><br><span class="line"> 10000                                                                column=f1:date, timestamp=2024-06-16T04:51:35.031, value=2014-12-05                                                                                                                                          </span><br><span class="line"> 10000                                                                column=f1:item_category, timestamp=2024-06-16T04:51:35.031, value=12488                                                                                                                                      </span><br><span class="line"> 10000                                                                column=f1:item_id, timestamp=2024-06-16T04:51:35.031, value=45571867                                                                                                                                         </span><br><span class="line"> 10000                                                                column=f1:province, timestamp=2024-06-16T04:51:35.031, value=\xE8\xB4\xB5\xE5\xB7\x9E                                                                                                                        </span><br><span class="line"> 10000                                                                column=f1:uid, timestamp=2024-06-16T04:51:35.031, value=100198255                                                                                                                                            </span><br><span class="line"> 100000                                                               column=f1:behavior_type, timestamp=2024-06-16T04:51:46.061, value=1                                                                                                                                          </span><br><span class="line"> 100000                                                               column=f1:date, timestamp=2024-06-16T04:51:46.061, value=2014-11-29                                                                                                                                          </span><br><span class="line"> 100000                                                               column=f1:item_category, timestamp=2024-06-16T04:51:46.061, value=6580                                                                                                                                       </span><br><span class="line"> 100000                                                               column=f1:item_id, timestamp=2024-06-16T04:51:46.061, value=78973192                                                                                                                                         </span><br><span class="line"> 100000                                                               column=f1:province, timestamp=2024-06-16T04:51:46.061, value=\xE5\xB1\xB1\xE4\xB8\x9C                                                                                                                        </span><br><span class="line"> 100000                                                               column=f1:uid, timestamp=2024-06-16T04:51:46.061, value=101480065                                                                                                                                            </span><br><span class="line"> 1000000                                                              column=f1:behavior_type, timestamp=2024-06-16T04:53:21.091, value=1                                                                                                                                          </span><br><span class="line"> 1000000                                                              column=f1:date, timestamp=2024-06-16T04:53:21.091, value=2014-11-29                                                                                                                                          </span><br><span class="line"> 1000000                                                              column=f1:item_category, timestamp=2024-06-16T04:53:21.091, value=10961                                                                                                                                      </span><br><span class="line"> 1000000                                                              column=f1:item_id, timestamp=2024-06-16T04:53:21.091, value=88763784                                                                                                                                         </span><br><span class="line"> 1000000                                                              column=f1:province, timestamp=2024-06-16T04:53:21.091, value=\xE5\xB1\xB1\xE8\xA5\xBF                                                                                                                        </span><br><span class="line"> 1000000                                                              column=f1:uid, timestamp=2024-06-16T04:53:21.091, value=111256954                                                                                                                                            </span><br><span class="line"> 10000000                                                             column=f1:behavior_type, timestamp=2024-06-16T05:09:10.864, value=1                                                                                                                                          </span><br><span class="line"> 10000000                                                             column=f1:date, timestamp=2024-06-16T05:09:10.864, value=2014-11-27                                                                                                                                          </span><br><span class="line"> 10000000                                                             column=f1:item_category, timestamp=2024-06-16T05:09:10.864, value=1854                                                                                                                                       </span><br><span class="line"> 10000000                                                             column=f1:item_id, timestamp=2024-06-16T05:09:10.864, value=87168683                                                                                                                                         </span><br><span class="line"> 10000000                                                             column=f1:province, timestamp=2024-06-16T05:09:10.864, value=\xE6\xB9\x96\xE5\x8D\x97                                                                                                                        </span><br><span class="line"> 10000000                                                             column=f1:uid, timestamp=2024-06-16T05:09:10.864, value=123265349                                                                                                                                            </span><br><span class="line"> 10000001                                                             column=f1:behavior_type, timestamp=2024-06-16T05:09:10.864, value=1                                                                                                                                          </span><br><span class="line"> 10000001                                                             column=f1:date, timestamp=2024-06-16T05:09:10.864, value=2014-11-18                                                                                                                                          </span><br><span class="line"> 10000001                                                             column=f1:item_category, timestamp=2024-06-16T05:09:10.864, value=13712                                                                                                                                      </span><br><span class="line"> 10000001                                                             column=f1:item_id, timestamp=2024-06-16T05:09:10.864, value=125559152                                                                                                                                        </span><br><span class="line"> 10000001                                                             column=f1:province, timestamp=2024-06-16T05:09:10.864, value=\xE6\xB9\x96\xE5\x8C\x97                                                                                                                        </span><br><span class="line"> 10000001                                                             column=f1:uid, timestamp=2024-06-16T05:09:10.864, value=123265349                                                                                                                                            </span><br><span class="line"> 10000002                                                             column=f1:behavior_type, timestamp=2024-06-16T05:09:10.864, value=1                                                                                                                                          </span><br><span class="line"> 10000002                                                             column=f1:date, timestamp=2024-06-16T05:09:10.864, value=2014-12-18                                                                                                                                          </span><br><span class="line"> 10000002                                                             column=f1:item_category, timestamp=2024-06-16T05:09:10.864, value=14079                                                                                                                                      </span><br><span class="line"> 10000002                                                             column=f1:item_id, timestamp=2024-06-16T05:09:10.864, value=222706509                                                                                                                                        </span><br><span class="line"> 10000002                                                             column=f1:province, timestamp=2024-06-16T05:09:10.864, value=\xE5\x86\x85\xE8\x92\x99\xE5\x8F\xA4                                                                                                            </span><br><span class="line"> 10000002                                                             column=f1:uid, timestamp=2024-06-16T05:09:10.864, value=123265349</span><br></pre></td></tr></table></figure>
<blockquote>
<p>没啦，别问为什么没写完，因为数据没导完啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊啊</p>
<p>不愧是大数据</p>
</blockquote>
<p>在这里放一只小猫，学累了就摸摸它</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">　　　　　　 ＿＿</span><br><span class="line">　　　　　／＞　　フ</span><br><span class="line">　　　　　| 　_　 _ l</span><br><span class="line">　 　　　／` ミ＿xノ</span><br><span class="line">　　 　 /　　　 　 |</span><br><span class="line">　　　 /　 ヽ　　 ﾉ</span><br><span class="line">　 　 │　　|　|　|</span><br><span class="line">　／￣|　　 |　|　|</span><br><span class="line">　| (￣ヽ＿_ヽ_)__)</span><br><span class="line">　＼二つ</span><br></pre></td></tr></table></figure>
<hr class="footnotes-sep" />
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://www.jetbrains.com/help/idea/installation-guide.html#-aa9dmj_178">Install IntelliJ IDEA</a> <a href="#fnref1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://mirrors.aliyun.com/eclipse/eclipse/downloads/drops4/R-4.23-202203080310/eclipse-SDK-4.23-linux-gtk-x86_64.tar.gz">Eclipse 阿里镜像站</a> <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://www.jetbrains.com/help/idea/working-with-module-dependencies.html#add-a-new-dependency">Add a new dependency</a> <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p><a target="_blank" rel="noopener" href="https://www.jetbrains.com/help/idea/working-with-artifacts.html#configure_artifact">Deployment/Artifacts</a> <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>

    </div>

    
    
    

    <footer class="post-footer">
          <div class="reward-container">
  <div>Buy me a coffee</div>
  <button>
    赞赏
  </button>
  <div class="post-reward">
      <div>
        <img src="/images/wechatpay.jpg" alt="Jccc 微信">
        <span>微信</span>
      </div>
      <div>
        <img src="/images/alipay.jpg" alt="Jccc 支付宝">
        <span>支付宝</span>
      </div>

  </div>
</div>

          

<div class="post-copyright">
<ul>
  <li class="post-copyright-author">
      <strong>本文作者： </strong>Jccc
  </li>
  <li class="post-copyright-link">
      <strong>本文链接：</strong>
      <a href="https://jccc-l.github.io/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-6%E2%80%94%E2%80%94%E5%AE%9E%E9%AA%8C%E6%A1%88%E4%BE%8B%EF%BC%9A%E7%BD%91%E7%AB%99%E7%94%A8%E6%88%B7%E8%A1%8C%E4%B8%BA%E5%88%86%E6%9E%90/" title="大数据实验踩坑指南_No.6——实验案例：网站用户行为分析">https://jccc-l.github.io/2024/06/16/大数据实验踩坑指南-No-6——实验案例：网站用户行为分析/</a>
  </li>
  <li class="post-copyright-license">
      <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/en" rel="noopener" target="_blank"><i class="fab fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>

          <div class="post-tags">
              <a href="/tags/Linux/" rel="tag"><i class="fa fa-tag"></i> Linux</a>
              <a href="/tags/Big-Data/" rel="tag"><i class="fa fa-tag"></i> Big Data</a>
              <a href="/tags/Hadoop/" rel="tag"><i class="fa fa-tag"></i> Hadoop</a>
              <a href="/tags/ZooKeeper/" rel="tag"><i class="fa fa-tag"></i> ZooKeeper</a>
              <a href="/tags/HBase/" rel="tag"><i class="fa fa-tag"></i> HBase</a>
              <a href="/tags/R/" rel="tag"><i class="fa fa-tag"></i> R</a>
              <a href="/tags/Hive/" rel="tag"><i class="fa fa-tag"></i> Hive</a>
              <a href="/tags/Java/" rel="tag"><i class="fa fa-tag"></i> Java</a>
          </div>

        

          <div class="post-nav">
            <div class="post-nav-item">
                <a href="/2024/06/16/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%AE%9E%E9%AA%8C%E8%B8%A9%E5%9D%91%E6%8C%87%E5%8D%97-No-5%E2%80%94%E2%80%94R%E8%AF%AD%E8%A8%80%E5%92%8C%E8%BD%AF%E4%BB%B6%E5%8C%85%E7%9A%84%E5%AE%89%E8%A3%85/" rel="prev" title="大数据实验踩坑指南_No.5——R语言和软件包的安装">
                  <i class="fa fa-angle-left"></i> 大数据实验踩坑指南_No.5——R语言和软件包的安装
                </a>
            </div>
            <div class="post-nav-item">
                <a href="/2024/06/17/Ubuntu%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AEWaydroid/" rel="next" title="Ubuntu安装配置Waydroid">
                  Ubuntu安装配置Waydroid <i class="fa fa-angle-right"></i>
                </a>
            </div>
          </div>
    </footer>
  </article>
</div>






</div>
  </main>

  <footer class="footer">
    <div class="footer-inner">

  <div class="copyright">
    &copy; 2023 – 
    <span itemprop="copyrightYear">2024</span>
    <span class="with-love">
      <i class="fa fa-heart"></i>
    </span>
    <span class="author" itemprop="copyrightHolder"><span id="copyright"> 路过的即是风景. All rights reserved. Non-commercial use allowed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a>.</span></span>
  </div>
<div class="wordcount">
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-chart-line"></i>
    </span>
    <span title="站点总字数">124k</span>
  </span>
  <span class="post-meta-item">
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:53</span>
  </span>
</div><footer>
    <div>
        <!-- 时间统计部分 -->
        <span id="timeDate">载入天数...</span>
        <span id="times">载入时分秒...</span>
        <script>
            var now = new Date();
            function createtime() {
                var grt = new Date("05/04/2023 00:00:00");
                now.setTime(now.getTime() + 250);
                days = (now - grt) / 1000 / 60 / 60 / 24; 
                dnum = Math.floor(days);
                hours = (now - grt) / 1000 / 60 / 60 - (24 * dnum); 
                hnum = Math.floor(hours);
                if(String(hnum).length == 1) {hnum = "0" + hnum;}
                minutes = (now - grt) / 1000 / 60 - (24 * 60 * dnum) - (60 * hnum);
                mnum = Math.floor(minutes);
                if(String(mnum).length == 1) {mnum = "0" + mnum;}
                seconds = (now - grt) / 1000 - (24 * 60 * 60 * dnum) - (60 * 60 * hnum) - (60 * mnum);
                snum = Math.round(seconds);
                if(String(snum).length == 1) {snum = "0" + snum;}
                document.getElementById("timeDate").innerHTML = "本站已安全运行 " + dnum + " 天 ";
                document.getElementById("times").innerHTML = hnum + " 小时 " + mnum + " 分 " + snum + " 秒";
            }
            setInterval("createtime()", 250);
        </script>
        <!-- 版权信息与联系信息部分 -->
<!--
		&copy; 2023-<span id="currentYearPlaceholder">YYYY</span>
		<span id="copyright"> 路过的即是风景. All rights reserved. Non-commercial use allowed under <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" target="_blank">CC BY-NC-SA 4.0</a>.</span>
-->
<!--
		<span id="disclaimer" style="text-align: left;">disclaimer: <br>content provided for reference only. <br>accuracy, completeness, suitability not guaranteed. <br>not liable for any loss or damage.</span>
-->

		<br>
		<!--联系信息部分-->
		<span id="Contact">Contact Me: <a href="mailto:1216550215Jc@gmail.com">GMail</a> | Address: Guangdong, China</span>
		<script>
			// 使用JavaScript获取当前年份并更新到页面上
			document.getElementById('currentYearPlaceholder').innerText = new Date().getFullYear();
		</script>
    </div>
</footer>


    </div>
  </footer>

  
  <div class="toggle sidebar-toggle" role="button">
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
    <span class="toggle-line"></span>
  </div>
  <div class="sidebar-dimmer"></div>
  <div class="reading-progress-bar"></div>

<noscript>
  <div class="noscript-warning">Theme NexT works best with JavaScript enabled</div>
</noscript>


  
  <script src="https://cdnjs.cloudflare.com/ajax/libs/animejs/3.2.1/anime.min.js" integrity="sha256-XL2inqUJaslATFnHdJOi9GfQ60on8Wx1C2H8DYiN1xY=" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/next-theme-pjax/0.6.0/pjax.min.js" integrity="sha256-vxLn1tSKWD4dqbMRyv940UYw4sXgMtYcK6reefzZrao=" crossorigin="anonymous"></script>
<script src="/js/comments.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/sidebar.js"></script><script src="/js/next-boot.js"></script><script src="/js/pjax.js"></script>

  <script src="https://cdnjs.cloudflare.com/ajax/libs/hexo-generator-searchdb/1.4.1/search.js" integrity="sha256-1kfA5uHPf65M5cphT2dvymhkuyHPQp5A53EGZOnOLmc=" crossorigin="anonymous"></script>
<script src="/js/third-party/search/local-search.js"></script>





  <script src="/js/third-party/pace.js"></script>


  




  

  <script class="next-config" data-name="enableMath" type="application/json">false</script><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.9/katex.min.css" integrity="sha256-UF1fgpAiu3tPJN/uCqEUHNe7pnr+QR0SQDNfgglgtcM=" crossorigin="anonymous">



</body>
</html>
